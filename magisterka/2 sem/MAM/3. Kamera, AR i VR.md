# 1\. **Interfejsy dostępu do kamery w Androidzie**

## 1.1. Camera (API przed Androidem 5.0)

- Stary, przestarzały interfejs (deprecated).
    
- Dostępny tylko przed Android 5.0 (API 21).
    
- Ograniczone możliwości: brak kontroli czasu ekspozycji, RAW itp.
    

## 1.2. Camera2 (nowoczesny interfejs)

- Precyzyjna kontrola parametrów:
    
    - czas naświetlania, focus, ISO, RAW.
- Możliwość równoległego strumieniowania z **kilku kamer jednocześnie**.
    
- Duża wydajność → np. nagrywanie wideo 8K.
    
- W Camera2 każda klatka przechodzi przez „pipeline”:
    
    - jeśli przetwarzanie nie nadąża → gubienie klatek.
- Przykłady zastosowań równoległych potoków:
    
    - zapis + podgląd (nagrywanie),
        
    - podgląd + rozpoznawanie kodów kreskowych,
        
    - podgląd + wykrywanie twarzy.
        

## 1.3. CameraX (warstwa wysokiego poziomu – Jetpack)

- Biblioteka Android Jetpack, zbudowana na Camera2.
    
- Dużo prostsza w użyciu.
    
- Wprowadza **use cases**:
    
    - *Preview* – podgląd na żywo,
        
    - *Image Analysis* – przetwarzanie obrazu (np. AI),
        
    - *Image / Video Capture* – zapis zdjęcia lub wideo.
        
- Abstrakcja pojedynczej operacji zamiast szczegółów niskiego poziomu.
    

* * *

# 2\. **AndroidX i Jetpack – kontekst działania CameraX**

## 2.1. AndroidX

- Zestaw bibliotek androidx.\* rozwijanych niezależnie od systemu Android.
    
- Jest następcą Android Support Library.
    
- Pozwala dostarczać nowe API bez aktualizacji systemu.
    

## 2.2. Jetpack

- Zestaw wysokopoziomowych bibliotek: Lifecycle, Navigation, Room, CameraX itd.
    
- Łatwiejsze budowanie nowoczesnych aplikacji.
    
- Umożliwia użycie funkcjonalności nowszych systemów na starszych API.
    

* * *

# 3\. **Kamery fizyczne i logiczne**

## 3.1. Kamera fizyczna

- Pojedynczy obiektyw + sensor.

## 3.2. Kamera logiczna

- Zestaw zgrupowanych kamer fizycznych:
    
    - mechanizm łączenia obrazu,
        
    - płynne przełączanie w zależności od zoomu,
        
    - dostępny jednolity interfejs.
        
- Od Android 9 (API 28) producenci muszą udostępniać logiczne kamery per grupa.
    
- Unika się „ukrytych” API producentów – standardyzacja.
    

* * *

# 4\. **Przykładowy przepływ korzystania z CameraX**

## 4.1. Deklaracja uprawnień

W AndroidManifest.xml (przed `<application>`):

`<uses-permission android:name="android.permission.CAMERA" /><uses-feature android:name="android.hardware.camera.any" android:required="true" />`

## 4.2. Żądanie uprawnień w czasie działania aplikacji

Sprawdzanie + żądanie:

`if (checkSelfPermission(CAMERA) == GRANTED) { startCamera()} else { requestCameraPermissionLauncher.launch(CAMERA)}`

Obsługa wyniku:

`registerForActivityResult(RequestPermission()) { isGranted -> ... }`

## 4.3. Uruchomienie podglądu (Preview)

1.  W XML używamy `PreviewView`.
    
2.  W kodzie:
    
    - pobieramy `ProcessCameraProvider`,
        
    - tworzymy obiekt `Preview`,
        
    - podpinamy `surfaceProvider`,
        
    - wybieramy kamerę (np. tylną),
        
    - `bindToLifecycle()`.
        

* * *

# 5\. **Kamera w iOS – porównanie**

## 5.1. Podstawy

- iOS korzysta z **AVCaptureSession**.
    
- Od iOS 13 – możliwość pracy wielu kamer i mikrofonów jednocześnie.
    

## 5.2. Uprawnienia

- Deklaracja w *Info.plist*:
    
    - `NSCameraUsageDescription` + opis.
- Możliwość dodania tłumaczeń w `InfoPlist.strings`.
    

* * *

# 6\. **Rzeczywistość rozszerzona – wprowadzenie**

## 6.1. Definicja

AR (Augmented Reality) to:

- nakładanie elementów wirtualnych na świat rzeczywisty,
    
- wymaga:
    
    - kamery,
        
    - wyświetlacza,
        
    - sensorów ruchu i położenia,
        
    - CPU oraz GPU.
        

Obrazy na stronach 12 i 13 pokazują przykłady modeli 3D wprowadzonych do obrazu kamery.

* * *

# 7\. **AR a VR – różnice**

## 7.1. Augmented Reality (AR)

- Realny świat + wirtualne elementy,
    
- użytkownik nadal widzi otoczenie.
    

## 7.2. Virtual Reality (VR)

- Całkowicie wirtualny świat 3D,
    
- rzeczywisty świat jest zasłonięty.
    

## 7.3. Mixed Reality (MR)

- Połączenie rzeczywistości i elementów 3D z interakcją (np. HoloLens).
    
- Przykłady:
    
    - Head-up display,
        
    - Head-mounted display.
        

* * *

# 8\. **Markery / znaczniki (fiducial markers)**

## 8.1. Czym są?

- Działają podobnie jak kody QR — rozpoznawane przez algorytmy.
    
- Pozwalają określić:
    
    - pozycję markera,
        
    - orientację markera → umożliwia osadzenie obiektu 3D.
        
- Przed odczytem konieczna jest transformacja rzutowania kamery.
    

## 8.2. Popularne rodziny markerów

- **ARToolKit**
    
- **ARTag**
    
- **AprilTag**
    
- **ArUco**
    

Grafika (strona 14) porównuje przykładowe markery.

* * *

# 9\. **Przykładowe zastosowania AR**

## 9.1. Przymierzanie zegarków (marker-based)

MAM_W03 2024

Model zegarka nakładany na obraz ręki użytkownika.

## 9.2. Przymierzanie okularów

System dopasowuje wirtualne oprawki do twarzy użytkownika.

## 9.3. Przymierzanie ubrań

Systemy rozpoznające sylwetkę i nakładające ubrania (str. 17).

## 9.4. AR w handlu – dodatki, buty

„Wanna Kicks” – aplikacja do przymierzania butów.

## 9.5. Wizualizacja mebli (IKEA Place)

Użytkownik może sprawdzić, jak mebel wygląda w mieszkaniu.

## 9.6. AR w grach

- **Pokemon GO**
    
- **The Witcher: Monster Slayer**
    

* * *

# 10\. **Biblioteki i narzędzia do AR**

## 10.1. ARKit (Apple)

- tylko iOS 11+,
    
- wymaga płatnego Apple Developer Program.
    

## 10.2. ARCore (Google)

- dla Androida 7.0+ oraz iOS 11+,
    
- darmowe,
    
- szeroko wykorzystywane w mobilnym AR.
    

## 10.3. Silniki 3D

- Unity, Unreal Engine, Godot, Vuforia.
    
- Często korzystają z ARKit lub ARCore.
    
- Wieloplatformowe.
    

* * *

# 11\. **Techniki AR — kluczowe koncepcje**

## 11.1. SLAM – Simultaneous Localization and Mapping

- Algorytm do rozpoznawania otoczenia i jego struktury.
    
- Buduje lokalną mapę punktów → wykrywa płaskie powierzchnie.
    
- Niezbędny do osadzania obiektów 3D na stole / podłodze.
    

## 11.2. Depth API (ARCore)

Depth API umożliwia:

- obliczanie głębi z wielu klatek obrazu,
    
- określenie odległości każdego piksela od kamery,
    
- integrację z czujnikiem głębi (jeśli dostępny).  
    Najlepsza dokładność: 0.5–5 m.
    

Ilustracja (str. 23) pokazuje mapę głębi.

## 11.3. Occlusion (zasłanianie)

- Wirtualny obiekt może być **zasłonięty** przez obiekt rzeczywisty.
    
- Kluczowy element realizmu AR.
    

## 11.4. Szacowanie oświetlenia

- ARCore potrafi generować:
    
    - światło kierunkowe,
        
    - światło otoczenia,
        
    - Mapę HDR środowiska.
        
- Dzięki temu obiekty wirtualne mają realistyczne cienie i odbicia.